{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Project  - Book 1: Classifying real vs fake sneakers via images\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "The sneaker resale market is an estimated 2 billion USD in secondary market in 2019. Estimated to be USD6 billion by 2025 according to research firm, Cowen & Co. Due to the lucrative nature of these commodities, there is the inevitible rise of counterfeits. The counterfeit sneakers industry are a USD450 million market and we want to be able to differentiate real and fake sneakers. \n",
    "\n",
    "Our task is to build a classifier that is able to differentiate between real and fake sneakers. \n",
    "Our primary audience will be the sneaker brands. Some of the negative impacts of counterfeit sneakers includes undercutting sales of brands, damaging reputation and dealing with the lashback from consumers.\n",
    "\n",
    "To do so, we will first be scrapping data from reddit and other sneaker resources and using classification models such as CNN and xxxx to diffentiate between the authentic and the replicas. We will measure our success using several classification metrics including xxxx and yyyy. \n",
    "\n",
    "With this, we also hope to help buyers inform themselves and to stay away from counterfeits. Empowering the public with information, they will be able to make the right decision which could help to reduce the lucrative nature of fake sneakers. \n",
    "\n",
    "\n",
    "## Executive Summary\n",
    "\n",
    "As the data science team in Nutrino, we have been tasked to build a classifier to improve core product of the company, which is to provide nutrition related data services and analytics. We are also tasked to identify patterns on 2 currently trending diets, keto and vegan. \n",
    "\n",
    "Our classifier was successful in predicting at an above 90% accuracy score. We also identified patterns in the motivations and preferences of the 2 groups of subredditors, which will help determine the kind of customer engagement with teach group. \n",
    "\n",
    "\n",
    "## Notebooks:\n",
    "- [Data Scrapping and Cleaning](./book1_data_scrapping_cleaning.ipynb)\n",
    "- [EDA](./book2_eda.ipynb)\n",
    "- [Modeling and Recommendations](./book3_preprocesing_modeling_recommendations.ipynb)\n",
    "\n",
    "\n",
    "## Contents:\n",
    "- [Import Libraries](#Import-Libraries)\n",
    "- [Data Scrapping](#Data-Scrapping)\n",
    "- [Data Cleaning](#Data-Cleaning)\n",
    "- [Save Data to CSV](#Save-Data-to-CSV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Scrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get data from subreddits\n",
    "\n",
    "Lucky for us, imgur is able to display images grouped by subreddit. We will be using their json API to retrieve links for the images we need. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#give list of sub reddits\n",
    "sub_reds = [\"repsneakers\",\"sneakermarket\"]\n",
    "\n",
    "#create lists to store scrapped data\n",
    "image_url = []\n",
    "rep_label = []\n",
    "\n",
    "imgur     = 'http://i.imgur.com/{}{}'\n",
    "page_api  = 'http://imgur.com/r/{}/new/page/{}/hit.json'\n",
    "album_api = 'http://imgur.com/ajaxalbums/getimages/{}/hit.json'\n",
    "\n",
    "\n",
    "for sub_red in sub_reds:\n",
    "    s = requests.session()\n",
    "    s.headers['user-agent'] = 'Mozilla/5.0'\n",
    "\n",
    "    for i in range(2):\n",
    "        url = page_api.format(sub_red,i)\n",
    "\n",
    "        j = s.get(url).json()\n",
    "        for entry in j['data']:\n",
    "            if entry['ext'] == '.gif' or entry['ext'] == '.mp4':\n",
    "                continue\n",
    "            else:\n",
    "                if entry['is_album']:\n",
    "                    url = album_api.format(entry['hash'])\n",
    "                    j = s.get(url).json()\n",
    "                    for image in j['data']['images']:\n",
    "                        if entry['ext'] == '.gif' or entry['ext'] == '.mp4':\n",
    "                            continue\n",
    "                        else:\n",
    "                            url = imgur.format(image['hash'], image['ext'])\n",
    "                            image_url.append(url)\n",
    "                            rep_label.append(sub_red)\n",
    "                    else:\n",
    "                        url = imgur.format(entry['hash'], entry['ext'])\n",
    "                        image_url.append(url)\n",
    "                        rep_label.append(sub_red)\n",
    "                        \n",
    "#credit: https://kaijento.github.io/2017/05/08/web-scraping-imgur.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1423\n",
      "1423\n"
     ]
    }
   ],
   "source": [
    "#here we check the number of urls we have scrapped\n",
    "print(len(image_url))\n",
    "print(len(rep_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "# Adding information about user agent\n",
    "opener=urllib.request.build_opener()\n",
    "opener.addheaders=[('User-Agent','Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1941.0 Safari/537.36')]\n",
    "urllib.request.install_opener(opener)\n",
    "\n",
    "for url in image_url[:100]:\n",
    "    dl_url = url\n",
    "    dl_name = dl_url.split(\"/\")[-1]\n",
    "\n",
    "    urllib.request.urlretrieve(dl_url,f'./datasets/images/{dl_name}')\n",
    "    \n",
    "    \n",
    "#credit: https://towardsdatascience.com/how-to-download-an-image-using-python-38a75cfa21c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#for url in image_url\n",
    "\n",
    "dl_url = image_url[0]\n",
    "dl_name = dl_url.split(\"/\")[-1]\n",
    "\n",
    "r = requests.get(dl_url, stream=True)\n",
    "\n",
    "if r.status_code == 200:\n",
    "    r.raw.decode_contnet = True\n",
    "    \n",
    "    with open(dl_name,'wb') as f:\n",
    "        shutil.copyfileobj(r.raw,f)\n",
    "    \n",
    "        \n",
    "    print(f\"Download Complete: {dl_name}\")\n",
    "    \n",
    "else: \n",
    "    print(f\"Download failed: {dl_name}\")\n",
    "    \n",
    "\n",
    "#credit: https://towardsdatascience.com/how-to-download-an-image-using-python-38a75cfa21c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list(zip(image_url,rep_label)),columns=['url','label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "repsneakers      1351\n",
       "sneakermarket      72\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_csv('./datasets/url.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
