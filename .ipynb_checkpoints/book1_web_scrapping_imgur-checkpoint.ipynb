{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Project  - Book 1: Classifying real vs fake sneakers via images\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "The sneaker resale market is an estimated 2 billion USD in secondary market in 2019. Estimated to be USD6 billion by 2025 according to research firm, Cowen & Co. Due to the lucrative nature of these commodities, there is the inevitible rise of counterfeits. The counterfeit sneakers industry are a USD450 million market and we want to be able to differentiate real and fake sneakers. \n",
    "\n",
    "Our task is to build a classifier that is able to differentiate between real and fake sneakers. \n",
    "Our primary audience will be the sneaker brands. Some of the negative impacts of counterfeit sneakers includes undercutting sales of brands, damaging reputation and dealing with the lashback from consumers.\n",
    "\n",
    "To do so, we will first be scrapping data from reddit and other sneaker resources and using classification models such as CNN and xxxx to diffentiate between the authentic and the replicas. We will measure our success using several classification metrics including xxxx and yyyy. \n",
    "\n",
    "With this, we also hope to help buyers inform themselves and to stay away from counterfeits. Empowering the public with information, they will be able to make the right decision which could help to reduce the lucrative nature of fake sneakers. \n",
    "\n",
    "\n",
    "## Executive Summary\n",
    "\n",
    "As the data science team in Nutrino, we have been tasked to build a classifier to improve core product of the company, which is to provide nutrition related data services and analytics. We are also tasked to identify patterns on 2 currently trending diets, keto and vegan. \n",
    "\n",
    "Our classifier was successful in predicting at an above 90% accuracy score. We also identified patterns in the motivations and preferences of the 2 groups of subredditors, which will help determine the kind of customer engagement with teach group. \n",
    "\n",
    "\n",
    "## Notebooks:\n",
    "- [Data Scrapping and Cleaning](./book1_data_scrapping_cleaning.ipynb)\n",
    "- [EDA](./book2_eda.ipynb)\n",
    "- [Modeling and Recommendations](./book3_preprocesing_modeling_recommendations.ipynb)\n",
    "\n",
    "\n",
    "## Contents:\n",
    "- [Import Libraries](#Import-Libraries)\n",
    "- [Data Scrapping](#Data-Scrapping)\n",
    "- [Data Cleaning](#Data-Cleaning)\n",
    "- [Save Data to CSV](#Save-Data-to-CSV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Scrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get data from subreddits\n",
    "\n",
    "Lucky for us, imgur is able to display images grouped by subreddit. We will be using their json API to retrieve links for the images we need. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrieving url from page 1 of sneakermarket...\n",
      "retrieving url from page 2 of sneakermarket...\n",
      "retrieving url from page 3 of sneakermarket...\n",
      "retrieving url from page 4 of sneakermarket...\n",
      "retrieving url from page 5 of sneakermarket...\n",
      "retrieving url from page 6 of sneakermarket...\n",
      "retrieving url from page 7 of sneakermarket...\n",
      "retrieving url from page 8 of sneakermarket...\n",
      "retrieving url from page 9 of sneakermarket...\n",
      "retrieving url from page 10 of sneakermarket...\n",
      "retrieving url from page 11 of sneakermarket...\n",
      "retrieving url from page 12 of sneakermarket...\n",
      "retrieving url from page 13 of sneakermarket...\n",
      "retrieving url from page 14 of sneakermarket...\n",
      "retrieving url from page 15 of sneakermarket...\n",
      "retrieving url from page 16 of sneakermarket...\n",
      "retrieving url from page 17 of sneakermarket...\n",
      "retrieving url from page 18 of sneakermarket...\n",
      "retrieving url from page 19 of sneakermarket...\n",
      "retrieving url from page 20 of sneakermarket...\n",
      "retrieving url from page 1 of sneakers...\n",
      "retrieving url from page 2 of sneakers...\n",
      "retrieving url from page 3 of sneakers...\n",
      "retrieving url from page 4 of sneakers...\n",
      "retrieving url from page 5 of sneakers...\n",
      "retrieving url from page 6 of sneakers...\n",
      "retrieving url from page 7 of sneakers...\n",
      "retrieving url from page 8 of sneakers...\n",
      "retrieving url from page 9 of sneakers...\n",
      "retrieving url from page 10 of sneakers...\n",
      "retrieving url from page 11 of sneakers...\n",
      "retrieving url from page 12 of sneakers...\n",
      "retrieving url from page 13 of sneakers...\n",
      "retrieving url from page 14 of sneakers...\n",
      "retrieving url from page 15 of sneakers...\n",
      "retrieving url from page 16 of sneakers...\n",
      "retrieving url from page 17 of sneakers...\n",
      "retrieving url from page 18 of sneakers...\n",
      "retrieving url from page 19 of sneakers...\n",
      "retrieving url from page 20 of sneakers...\n",
      "retrieving url from page 1 of sneakerhead...\n",
      "retrieving url from page 2 of sneakerhead...\n",
      "retrieving url from page 3 of sneakerhead...\n",
      "retrieving url from page 4 of sneakerhead...\n",
      "retrieving url from page 5 of sneakerhead...\n",
      "retrieving url from page 6 of sneakerhead...\n",
      "retrieving url from page 7 of sneakerhead...\n",
      "retrieving url from page 8 of sneakerhead...\n",
      "retrieving url from page 9 of sneakerhead...\n",
      "retrieving url from page 10 of sneakerhead...\n",
      "retrieving url from page 11 of sneakerhead...\n",
      "retrieving url from page 12 of sneakerhead...\n",
      "retrieving url from page 13 of sneakerhead...\n",
      "retrieving url from page 14 of sneakerhead...\n",
      "retrieving url from page 15 of sneakerhead...\n",
      "retrieving url from page 16 of sneakerhead...\n",
      "retrieving url from page 17 of sneakerhead...\n",
      "retrieving url from page 18 of sneakerhead...\n",
      "retrieving url from page 19 of sneakerhead...\n",
      "retrieving url from page 20 of sneakerhead...\n",
      "retrieving url from page 1 of sneakerscanada...\n",
      "retrieving url from page 2 of sneakerscanada...\n",
      "retrieving url from page 3 of sneakerscanada...\n",
      "retrieving url from page 4 of sneakerscanada...\n",
      "retrieving url from page 5 of sneakerscanada...\n",
      "retrieving url from page 6 of sneakerscanada...\n",
      "retrieving url from page 7 of sneakerscanada...\n",
      "retrieving url from page 8 of sneakerscanada...\n",
      "retrieving url from page 9 of sneakerscanada...\n",
      "retrieving url from page 10 of sneakerscanada...\n",
      "retrieving url from page 11 of sneakerscanada...\n",
      "retrieving url from page 12 of sneakerscanada...\n",
      "retrieving url from page 13 of sneakerscanada...\n",
      "retrieving url from page 14 of sneakerscanada...\n",
      "retrieving url from page 15 of sneakerscanada...\n",
      "retrieving url from page 16 of sneakerscanada...\n",
      "retrieving url from page 17 of sneakerscanada...\n",
      "retrieving url from page 18 of sneakerscanada...\n",
      "retrieving url from page 19 of sneakerscanada...\n",
      "retrieving url from page 20 of sneakerscanada...\n"
     ]
    }
   ],
   "source": [
    "#give list of sub reddits\n",
    "sub_reds = [\"repsneakers\",\"sneakerreps\", \"wengkksneakers\",\n",
    "            \"chanzhfsneakers\",\"michaelsneakers\",\"sneakermarket\",\n",
    "            \"sneakers\",\"sneakerhead\",\"sneakerscanada\"]\n",
    "\n",
    "#create lists to store scrapped data\n",
    "image_url = []\n",
    "rep_label = []\n",
    "\n",
    "#save links to variables\n",
    "imgur     = 'http://i.imgur.com/{}{}'\n",
    "page_api  = 'http://imgur.com/r/{}/new/page/{}/hit.json'\n",
    "album_api = 'http://imgur.com/ajaxalbums/getimages/{}/hit.json'\n",
    "\n",
    "\n",
    "for sub_red in sub_reds:                                 #iterate through the list of sub_reds\n",
    "    s = requests.session()                               #instantiate session\n",
    "    s.headers['user-agent'] = 'Mozilla/5.0'\n",
    "\n",
    "    for i in range(5):                                   #iterate through pages\n",
    "        url = page_api.format(sub_red,i)\n",
    "        print(f\"retrieving url from page {i+1} of {sub_red}...\")\n",
    "\n",
    "        j = s.get(url).json()\n",
    "        for entry in j['data']:                          #iterate through post in each page\n",
    "            if entry['ext'] == '.gif' or entry['ext'] == '.mp4':\n",
    "                pass                                     #if its a gif or video, pass\n",
    "            else:\n",
    "                if entry['is_album']:                    #check if its an album\n",
    "                    url = album_api.format(entry['hash'])\n",
    "                    j = s.get(url).json()\n",
    "                    for image in j['data']['images']:    #iterate through album\n",
    "                        if entry['ext'] == '.gif' or entry['ext'] == '.mp4':\n",
    "                            pass                         #if its a gif or video, pass\n",
    "                        else:\n",
    "                            url = imgur.format(image['hash'], image['ext'])\n",
    "                            image_url.append(url)        #if not append link to list\n",
    "                            rep_label.append(sub_red)    #add label\n",
    "                else:\n",
    "                    url = imgur.format(entry['hash'], entry['ext'])\n",
    "                    image_url.append(url)\n",
    "                    rep_label.append(sub_red)\n",
    "            \n",
    "\n",
    "#credit: https://kaijento.github.io/2017/05/08/web-scraping-imgur.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5564\n",
      "5564\n"
     ]
    }
   ],
   "source": [
    "#here we check the number of urls we have scrapped\n",
    "print(len(image_url))\n",
    "print(len(rep_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create labels for the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_rep = ['rep' if label in [\"repsneakers\", \"sneakerreps\", \"wengkksneakers\", \"chanzhfsneakers\", \"michaelsneakers\"] else 'auth' for label in rep_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5564"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(is_rep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5564, 3)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(list(zip(image_url,rep_label,is_rep)),columns=['url','label','is_rep'])\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>label</th>\n",
       "      <th>is_rep</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://i.imgur.com/v92pz5I.jpg</td>\n",
       "      <td>sneakermarket</td>\n",
       "      <td>auth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://i.imgur.com/VwpI9qg.jpg</td>\n",
       "      <td>sneakermarket</td>\n",
       "      <td>auth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://i.imgur.com/OgT4CwF.jpg</td>\n",
       "      <td>sneakermarket</td>\n",
       "      <td>auth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://i.imgur.com/RfeaaRK.jpg</td>\n",
       "      <td>sneakermarket</td>\n",
       "      <td>auth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://i.imgur.com/veiqN5R.jpg</td>\n",
       "      <td>sneakermarket</td>\n",
       "      <td>auth</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              url          label is_rep\n",
       "0  http://i.imgur.com/v92pz5I.jpg  sneakermarket   auth\n",
       "1  http://i.imgur.com/VwpI9qg.jpg  sneakermarket   auth\n",
       "2  http://i.imgur.com/OgT4CwF.jpg  sneakermarket   auth\n",
       "3  http://i.imgur.com/RfeaaRK.jpg  sneakermarket   auth\n",
       "4  http://i.imgur.com/veiqN5R.jpg  sneakermarket   auth"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sneakermarket     2500\n",
       "sneakers          1992\n",
       "sneakerscanada     704\n",
       "sneakerhead        368\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation 1 of the dataset\n",
    "We have a severely imbalanced class in repsneakers here. I will attempt to dig deeper into the sneakers and sneaker market subreddits for more images. \n",
    "\n",
    "This is likely because in r/repsneakers there are alot of people looking to \"quality control\" (QC) for the best replicas. Therefore, there are going to be more images and with better details. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates('url', inplace=True)\n",
    "\n",
    "df.reset_index(inplace=True)\n",
    "df.drop('index',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sneakermarket     625\n",
       "sneakers          498\n",
       "sneakerscanada    176\n",
       "sneakerhead        92\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "auth    1391\n",
       "Name: is_rep, dtype: int64"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['is_rep'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation 2 of the dataset\n",
    "\n",
    "1. There seems to be some duplicates in the data. Seems like there is a limit on the number of posts to be scrapped. I will be scraping for new posts everyday. \n",
    "2. Here we can also see that there is a major class imbalance. I will be looking for more authentic sneaker subreddits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only for downloading images\n",
    "df = pd.read_csv(\"./datasets/auth_url_1k.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>label</th>\n",
       "      <th>is_rep</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://i.imgur.com/v92pz5I.jpg</td>\n",
       "      <td>sneakermarket</td>\n",
       "      <td>auth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://i.imgur.com/VwpI9qg.jpg</td>\n",
       "      <td>sneakermarket</td>\n",
       "      <td>auth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://i.imgur.com/OgT4CwF.jpg</td>\n",
       "      <td>sneakermarket</td>\n",
       "      <td>auth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://i.imgur.com/RfeaaRK.jpg</td>\n",
       "      <td>sneakermarket</td>\n",
       "      <td>auth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://i.imgur.com/veiqN5R.jpg</td>\n",
       "      <td>sneakermarket</td>\n",
       "      <td>auth</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              url          label is_rep\n",
       "0  http://i.imgur.com/v92pz5I.jpg  sneakermarket   auth\n",
       "1  http://i.imgur.com/VwpI9qg.jpg  sneakermarket   auth\n",
       "2  http://i.imgur.com/OgT4CwF.jpg  sneakermarket   auth\n",
       "3  http://i.imgur.com/RfeaaRK.jpg  sneakermarket   auth\n",
       "4  http://i.imgur.com/veiqN5R.jpg  sneakermarket   auth"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading 912 of 1391 images...\n",
      "downloading 913 of 1391 images...\n",
      "downloading 914 of 1391 images...\n",
      "downloading 915 of 1391 images...\n",
      "downloading 916 of 1391 images...\n",
      "downloading 917 of 1391 images...\n",
      "downloading 918 of 1391 images...\n",
      "downloading 919 of 1391 images...\n",
      "downloading 920 of 1391 images...\n",
      "downloading 921 of 1391 images...\n",
      "downloading 922 of 1391 images...\n",
      "downloading 923 of 1391 images...\n",
      "downloading 924 of 1391 images...\n",
      "downloading 925 of 1391 images...\n",
      "downloading 926 of 1391 images...\n",
      "downloading 927 of 1391 images...\n",
      "downloading 928 of 1391 images...\n",
      "downloading 929 of 1391 images...\n",
      "downloading 930 of 1391 images...\n",
      "downloading 931 of 1391 images...\n",
      "downloading 932 of 1391 images...\n",
      "downloading 933 of 1391 images...\n",
      "downloading 934 of 1391 images...\n",
      "downloading 935 of 1391 images...\n",
      "downloading 936 of 1391 images...\n",
      "downloading 937 of 1391 images...\n",
      "downloading 938 of 1391 images...\n",
      "downloading 939 of 1391 images...\n",
      "downloading 940 of 1391 images...\n",
      "downloading 941 of 1391 images...\n",
      "downloading 942 of 1391 images...\n",
      "downloading 943 of 1391 images...\n",
      "downloading 944 of 1391 images...\n",
      "downloading 945 of 1391 images...\n",
      "downloading 946 of 1391 images...\n",
      "downloading 947 of 1391 images...\n",
      "downloading 948 of 1391 images...\n",
      "downloading 949 of 1391 images...\n",
      "downloading 950 of 1391 images...\n",
      "downloading 951 of 1391 images...\n",
      "downloading 952 of 1391 images...\n",
      "downloading 953 of 1391 images...\n",
      "downloading 954 of 1391 images...\n",
      "downloading 955 of 1391 images...\n",
      "downloading 956 of 1391 images...\n",
      "downloading 957 of 1391 images...\n",
      "downloading 958 of 1391 images...\n",
      "downloading 959 of 1391 images...\n",
      "downloading 960 of 1391 images...\n",
      "downloading 961 of 1391 images...\n",
      "downloading 962 of 1391 images...\n",
      "downloading 963 of 1391 images...\n",
      "downloading 964 of 1391 images...\n",
      "downloading 965 of 1391 images...\n",
      "downloading 966 of 1391 images...\n",
      "downloading 967 of 1391 images...\n",
      "downloading 968 of 1391 images...\n",
      "downloading 969 of 1391 images...\n",
      "downloading 970 of 1391 images...\n",
      "downloading 971 of 1391 images...\n",
      "downloading 972 of 1391 images...\n",
      "downloading 973 of 1391 images...\n",
      "downloading 974 of 1391 images...\n",
      "downloading 975 of 1391 images...\n",
      "downloading 976 of 1391 images...\n",
      "downloading 977 of 1391 images...\n",
      "downloading 978 of 1391 images...\n",
      "downloading 979 of 1391 images...\n",
      "downloading 980 of 1391 images...\n",
      "downloading 981 of 1391 images...\n",
      "downloading 982 of 1391 images...\n",
      "downloading 983 of 1391 images...\n",
      "downloading 984 of 1391 images...\n",
      "downloading 985 of 1391 images...\n",
      "downloading 986 of 1391 images...\n",
      "downloading 987 of 1391 images...\n",
      "downloading 988 of 1391 images...\n",
      "downloading 989 of 1391 images...\n",
      "downloading 990 of 1391 images...\n",
      "downloading 991 of 1391 images...\n",
      "downloading 992 of 1391 images...\n",
      "downloading 993 of 1391 images...\n",
      "downloading 994 of 1391 images...\n",
      "downloading 995 of 1391 images...\n",
      "downloading 996 of 1391 images...\n",
      "downloading 997 of 1391 images...\n",
      "downloading 998 of 1391 images...\n",
      "downloading 999 of 1391 images...\n",
      "downloading 1000 of 1391 images...\n",
      "downloading 1001 of 1391 images...\n",
      "downloading 1002 of 1391 images...\n",
      "downloading 1003 of 1391 images...\n",
      "downloading 1004 of 1391 images...\n",
      "downloading 1005 of 1391 images...\n",
      "downloading 1006 of 1391 images...\n",
      "downloading 1007 of 1391 images...\n",
      "downloading 1008 of 1391 images...\n",
      "downloading 1009 of 1391 images...\n",
      "downloading 1010 of 1391 images...\n",
      "downloading 1011 of 1391 images...\n",
      "downloading 1012 of 1391 images...\n",
      "downloading 1013 of 1391 images...\n",
      "downloading 1014 of 1391 images...\n",
      "downloading 1015 of 1391 images...\n",
      "downloading 1016 of 1391 images...\n",
      "downloading 1017 of 1391 images...\n",
      "downloading 1018 of 1391 images...\n",
      "downloading 1019 of 1391 images...\n",
      "downloading 1020 of 1391 images...\n",
      "downloading 1021 of 1391 images...\n",
      "downloading 1022 of 1391 images...\n",
      "downloading 1023 of 1391 images...\n",
      "downloading 1024 of 1391 images...\n",
      "downloading 1025 of 1391 images...\n",
      "downloading 1026 of 1391 images...\n",
      "downloading 1027 of 1391 images...\n",
      "downloading 1028 of 1391 images...\n",
      "downloading 1029 of 1391 images...\n",
      "downloading 1030 of 1391 images...\n",
      "downloading 1031 of 1391 images...\n",
      "downloading 1032 of 1391 images...\n",
      "downloading 1033 of 1391 images...\n",
      "downloading 1034 of 1391 images...\n",
      "downloading 1035 of 1391 images...\n",
      "downloading 1036 of 1391 images...\n",
      "downloading 1037 of 1391 images...\n",
      "downloading 1038 of 1391 images...\n",
      "downloading 1039 of 1391 images...\n",
      "downloading 1040 of 1391 images...\n",
      "downloading 1041 of 1391 images...\n",
      "downloading 1042 of 1391 images...\n",
      "downloading 1043 of 1391 images...\n",
      "downloading 1044 of 1391 images...\n",
      "downloading 1045 of 1391 images...\n",
      "downloading 1046 of 1391 images...\n",
      "downloading 1047 of 1391 images...\n",
      "downloading 1048 of 1391 images...\n",
      "downloading 1049 of 1391 images...\n",
      "downloading 1050 of 1391 images...\n",
      "downloading 1051 of 1391 images...\n",
      "downloading 1052 of 1391 images...\n",
      "downloading 1053 of 1391 images...\n",
      "downloading 1054 of 1391 images...\n",
      "downloading 1055 of 1391 images...\n",
      "downloading 1056 of 1391 images...\n",
      "downloading 1057 of 1391 images...\n",
      "downloading 1058 of 1391 images...\n",
      "downloading 1059 of 1391 images...\n",
      "downloading 1060 of 1391 images...\n",
      "downloading 1061 of 1391 images...\n",
      "downloading 1062 of 1391 images...\n",
      "downloading 1063 of 1391 images...\n",
      "downloading 1064 of 1391 images...\n",
      "downloading 1065 of 1391 images...\n",
      "downloading 1066 of 1391 images...\n",
      "downloading 1067 of 1391 images...\n",
      "downloading 1068 of 1391 images...\n",
      "downloading 1069 of 1391 images...\n",
      "downloading 1070 of 1391 images...\n",
      "downloading 1071 of 1391 images...\n",
      "downloading 1072 of 1391 images...\n",
      "downloading 1073 of 1391 images...\n",
      "downloading 1074 of 1391 images...\n",
      "downloading 1075 of 1391 images...\n",
      "downloading 1076 of 1391 images...\n",
      "downloading 1077 of 1391 images...\n",
      "downloading 1078 of 1391 images...\n",
      "downloading 1079 of 1391 images...\n",
      "downloading 1080 of 1391 images...\n",
      "downloading 1081 of 1391 images...\n",
      "downloading 1082 of 1391 images...\n",
      "downloading 1083 of 1391 images...\n",
      "downloading 1084 of 1391 images...\n",
      "downloading 1085 of 1391 images...\n",
      "downloading 1086 of 1391 images...\n",
      "downloading 1087 of 1391 images...\n",
      "downloading 1088 of 1391 images...\n",
      "downloading 1089 of 1391 images...\n",
      "downloading 1090 of 1391 images...\n",
      "downloading 1091 of 1391 images...\n",
      "downloading 1092 of 1391 images...\n",
      "downloading 1093 of 1391 images...\n",
      "downloading 1094 of 1391 images...\n",
      "downloading 1095 of 1391 images...\n",
      "downloading 1096 of 1391 images...\n",
      "downloading 1097 of 1391 images...\n",
      "downloading 1098 of 1391 images...\n",
      "downloading 1099 of 1391 images...\n",
      "downloading 1100 of 1391 images...\n",
      "downloading 1101 of 1391 images...\n",
      "downloading 1102 of 1391 images...\n",
      "downloading 1103 of 1391 images...\n",
      "downloading 1104 of 1391 images...\n",
      "downloading 1105 of 1391 images...\n",
      "downloading 1106 of 1391 images...\n",
      "downloading 1107 of 1391 images...\n",
      "downloading 1108 of 1391 images...\n",
      "downloading 1109 of 1391 images...\n",
      "downloading 1110 of 1391 images...\n",
      "downloading 1111 of 1391 images...\n",
      "downloading 1112 of 1391 images...\n",
      "downloading 1113 of 1391 images...\n",
      "downloading 1114 of 1391 images...\n",
      "downloading 1115 of 1391 images...\n",
      "downloading 1116 of 1391 images...\n",
      "downloading 1117 of 1391 images...\n",
      "downloading 1118 of 1391 images...\n",
      "downloading 1119 of 1391 images...\n",
      "downloading 1120 of 1391 images...\n",
      "downloading 1121 of 1391 images...\n",
      "downloading 1122 of 1391 images...\n",
      "downloading 1123 of 1391 images...\n",
      "downloading 1124 of 1391 images...\n",
      "downloading 1125 of 1391 images...\n",
      "downloading 1126 of 1391 images...\n",
      "downloading 1127 of 1391 images...\n",
      "downloading 1128 of 1391 images...\n",
      "downloading 1129 of 1391 images...\n",
      "downloading 1130 of 1391 images...\n",
      "downloading 1131 of 1391 images...\n",
      "downloading 1132 of 1391 images...\n",
      "downloading 1133 of 1391 images...\n",
      "downloading 1134 of 1391 images...\n",
      "downloading 1135 of 1391 images...\n",
      "downloading 1136 of 1391 images...\n",
      "downloading 1137 of 1391 images...\n",
      "downloading 1138 of 1391 images...\n",
      "downloading 1139 of 1391 images...\n",
      "downloading 1140 of 1391 images...\n",
      "downloading 1141 of 1391 images...\n",
      "downloading 1142 of 1391 images...\n",
      "downloading 1143 of 1391 images...\n",
      "downloading 1144 of 1391 images...\n",
      "downloading 1145 of 1391 images...\n",
      "downloading 1146 of 1391 images...\n",
      "downloading 1147 of 1391 images...\n",
      "downloading 1148 of 1391 images...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading 1149 of 1391 images...\n",
      "downloading 1150 of 1391 images...\n",
      "downloading 1151 of 1391 images...\n",
      "downloading 1152 of 1391 images...\n",
      "downloading 1153 of 1391 images...\n",
      "downloading 1154 of 1391 images...\n",
      "downloading 1155 of 1391 images...\n",
      "downloading 1156 of 1391 images...\n",
      "downloading 1157 of 1391 images...\n",
      "downloading 1158 of 1391 images...\n",
      "downloading 1159 of 1391 images...\n",
      "downloading 1160 of 1391 images...\n",
      "downloading 1161 of 1391 images...\n",
      "downloading 1162 of 1391 images...\n",
      "downloading 1163 of 1391 images...\n",
      "downloading 1164 of 1391 images...\n",
      "downloading 1165 of 1391 images...\n",
      "downloading 1166 of 1391 images...\n",
      "downloading 1167 of 1391 images...\n",
      "downloading 1168 of 1391 images...\n",
      "downloading 1169 of 1391 images...\n",
      "downloading 1170 of 1391 images...\n",
      "downloading 1171 of 1391 images...\n",
      "downloading 1172 of 1391 images...\n",
      "downloading 1173 of 1391 images...\n",
      "downloading 1174 of 1391 images...\n",
      "downloading 1175 of 1391 images...\n",
      "downloading 1176 of 1391 images...\n",
      "downloading 1177 of 1391 images...\n",
      "downloading 1178 of 1391 images...\n",
      "downloading 1179 of 1391 images...\n",
      "downloading 1180 of 1391 images...\n",
      "downloading 1181 of 1391 images...\n",
      "downloading 1182 of 1391 images...\n",
      "downloading 1183 of 1391 images...\n",
      "downloading 1184 of 1391 images...\n",
      "downloading 1185 of 1391 images...\n",
      "downloading 1186 of 1391 images...\n",
      "downloading 1187 of 1391 images...\n",
      "downloading 1188 of 1391 images...\n",
      "downloading 1189 of 1391 images...\n",
      "downloading 1190 of 1391 images...\n",
      "downloading 1191 of 1391 images...\n",
      "downloading 1192 of 1391 images...\n",
      "downloading 1193 of 1391 images...\n",
      "downloading 1194 of 1391 images...\n",
      "downloading 1195 of 1391 images...\n",
      "downloading 1196 of 1391 images...\n",
      "downloading 1197 of 1391 images...\n",
      "downloading 1198 of 1391 images...\n",
      "downloading 1199 of 1391 images...\n",
      "downloading 1200 of 1391 images...\n",
      "downloading 1201 of 1391 images...\n",
      "downloading 1202 of 1391 images...\n",
      "downloading 1203 of 1391 images...\n",
      "downloading 1204 of 1391 images...\n",
      "downloading 1205 of 1391 images...\n",
      "downloading 1206 of 1391 images...\n",
      "downloading 1207 of 1391 images...\n",
      "downloading 1208 of 1391 images...\n",
      "downloading 1209 of 1391 images...\n",
      "downloading 1210 of 1391 images...\n",
      "downloading 1211 of 1391 images...\n",
      "downloading 1212 of 1391 images...\n",
      "downloading 1213 of 1391 images...\n",
      "downloading 1214 of 1391 images...\n",
      "downloading 1215 of 1391 images...\n",
      "downloading 1216 of 1391 images...\n",
      "downloading 1217 of 1391 images...\n",
      "downloading 1218 of 1391 images...\n",
      "downloading 1219 of 1391 images...\n",
      "downloading 1220 of 1391 images...\n",
      "downloading 1221 of 1391 images...\n",
      "downloading 1222 of 1391 images...\n",
      "downloading 1223 of 1391 images...\n",
      "downloading 1224 of 1391 images...\n",
      "downloading 1225 of 1391 images...\n",
      "downloading 1226 of 1391 images...\n",
      "downloading 1227 of 1391 images...\n",
      "downloading 1228 of 1391 images...\n",
      "downloading 1229 of 1391 images...\n",
      "downloading 1230 of 1391 images...\n",
      "downloading 1231 of 1391 images...\n",
      "downloading 1232 of 1391 images...\n",
      "downloading 1233 of 1391 images...\n",
      "downloading 1234 of 1391 images...\n",
      "downloading 1235 of 1391 images...\n",
      "downloading 1236 of 1391 images...\n",
      "downloading 1237 of 1391 images...\n",
      "downloading 1238 of 1391 images...\n",
      "downloading 1239 of 1391 images...\n",
      "downloading 1240 of 1391 images...\n",
      "downloading 1241 of 1391 images...\n",
      "downloading 1242 of 1391 images...\n",
      "downloading 1243 of 1391 images...\n",
      "downloading 1244 of 1391 images...\n",
      "downloading 1245 of 1391 images...\n",
      "downloading 1246 of 1391 images...\n",
      "downloading 1247 of 1391 images...\n",
      "downloading 1248 of 1391 images...\n",
      "downloading 1249 of 1391 images...\n",
      "downloading 1250 of 1391 images...\n",
      "downloading 1251 of 1391 images...\n",
      "downloading 1252 of 1391 images...\n",
      "downloading 1253 of 1391 images...\n",
      "downloading 1254 of 1391 images...\n",
      "downloading 1255 of 1391 images...\n",
      "downloading 1256 of 1391 images...\n",
      "downloading 1257 of 1391 images...\n",
      "downloading 1258 of 1391 images...\n",
      "downloading 1259 of 1391 images...\n",
      "downloading 1260 of 1391 images...\n",
      "downloading 1261 of 1391 images...\n",
      "downloading 1262 of 1391 images...\n",
      "downloading 1263 of 1391 images...\n",
      "downloading 1264 of 1391 images...\n",
      "downloading 1265 of 1391 images...\n",
      "downloading 1266 of 1391 images...\n",
      "downloading 1267 of 1391 images...\n",
      "downloading 1268 of 1391 images...\n",
      "downloading 1269 of 1391 images...\n",
      "downloading 1270 of 1391 images...\n",
      "downloading 1271 of 1391 images...\n",
      "downloading 1272 of 1391 images...\n",
      "downloading 1273 of 1391 images...\n",
      "downloading 1274 of 1391 images...\n",
      "downloading 1275 of 1391 images...\n",
      "downloading 1276 of 1391 images...\n",
      "downloading 1277 of 1391 images...\n",
      "downloading 1278 of 1391 images...\n",
      "downloading 1279 of 1391 images...\n",
      "downloading 1280 of 1391 images...\n",
      "downloading 1281 of 1391 images...\n",
      "downloading 1282 of 1391 images...\n",
      "downloading 1283 of 1391 images...\n",
      "downloading 1284 of 1391 images...\n",
      "downloading 1285 of 1391 images...\n",
      "downloading 1286 of 1391 images...\n",
      "downloading 1287 of 1391 images...\n",
      "downloading 1288 of 1391 images...\n",
      "downloading 1289 of 1391 images...\n",
      "downloading 1290 of 1391 images...\n",
      "downloading 1291 of 1391 images...\n",
      "downloading 1292 of 1391 images...\n",
      "downloading 1293 of 1391 images...\n",
      "downloading 1294 of 1391 images...\n",
      "downloading 1295 of 1391 images...\n",
      "downloading 1296 of 1391 images...\n",
      "downloading 1297 of 1391 images...\n",
      "downloading 1298 of 1391 images...\n",
      "downloading 1299 of 1391 images...\n",
      "downloading 1300 of 1391 images...\n",
      "downloading 1301 of 1391 images...\n",
      "downloading 1302 of 1391 images...\n",
      "downloading 1303 of 1391 images...\n",
      "downloading 1304 of 1391 images...\n",
      "downloading 1305 of 1391 images...\n",
      "downloading 1306 of 1391 images...\n",
      "downloading 1307 of 1391 images...\n",
      "downloading 1308 of 1391 images...\n",
      "downloading 1309 of 1391 images...\n",
      "downloading 1310 of 1391 images...\n",
      "downloading 1311 of 1391 images...\n",
      "downloading 1312 of 1391 images...\n",
      "downloading 1313 of 1391 images...\n",
      "downloading 1314 of 1391 images...\n",
      "downloading 1315 of 1391 images...\n",
      "downloading 1316 of 1391 images...\n",
      "downloading 1317 of 1391 images...\n",
      "downloading 1318 of 1391 images...\n",
      "downloading 1319 of 1391 images...\n",
      "downloading 1320 of 1391 images...\n",
      "downloading 1321 of 1391 images...\n",
      "downloading 1322 of 1391 images...\n",
      "downloading 1323 of 1391 images...\n",
      "downloading 1324 of 1391 images...\n",
      "downloading 1325 of 1391 images...\n",
      "downloading 1326 of 1391 images...\n",
      "downloading 1327 of 1391 images...\n",
      "downloading 1328 of 1391 images...\n",
      "downloading 1329 of 1391 images...\n",
      "downloading 1330 of 1391 images...\n",
      "downloading 1331 of 1391 images...\n",
      "downloading 1332 of 1391 images...\n",
      "downloading 1333 of 1391 images...\n",
      "downloading 1334 of 1391 images...\n",
      "downloading 1335 of 1391 images...\n",
      "downloading 1336 of 1391 images...\n",
      "downloading 1337 of 1391 images...\n",
      "downloading 1338 of 1391 images...\n",
      "downloading 1339 of 1391 images...\n",
      "downloading 1340 of 1391 images...\n",
      "downloading 1341 of 1391 images...\n",
      "downloading 1342 of 1391 images...\n",
      "downloading 1343 of 1391 images...\n",
      "downloading 1344 of 1391 images...\n",
      "downloading 1345 of 1391 images...\n",
      "downloading 1346 of 1391 images...\n",
      "downloading 1347 of 1391 images...\n",
      "downloading 1348 of 1391 images...\n",
      "downloading 1349 of 1391 images...\n",
      "downloading 1350 of 1391 images...\n",
      "downloading 1351 of 1391 images...\n",
      "downloading 1352 of 1391 images...\n",
      "downloading 1353 of 1391 images...\n",
      "downloading 1354 of 1391 images...\n",
      "downloading 1355 of 1391 images...\n",
      "downloading 1356 of 1391 images...\n",
      "downloading 1357 of 1391 images...\n",
      "downloading 1358 of 1391 images...\n",
      "downloading 1359 of 1391 images...\n",
      "downloading 1360 of 1391 images...\n",
      "downloading 1361 of 1391 images...\n",
      "downloading 1362 of 1391 images...\n",
      "downloading 1363 of 1391 images...\n",
      "downloading 1364 of 1391 images...\n",
      "downloading 1365 of 1391 images...\n",
      "downloading 1366 of 1391 images...\n",
      "downloading 1367 of 1391 images...\n",
      "downloading 1368 of 1391 images...\n",
      "downloading 1369 of 1391 images...\n",
      "downloading 1370 of 1391 images...\n",
      "downloading 1371 of 1391 images...\n",
      "downloading 1372 of 1391 images...\n",
      "downloading 1373 of 1391 images...\n",
      "downloading 1374 of 1391 images...\n",
      "downloading 1375 of 1391 images...\n",
      "downloading 1376 of 1391 images...\n",
      "downloading 1377 of 1391 images...\n",
      "downloading 1378 of 1391 images...\n",
      "downloading 1379 of 1391 images...\n",
      "downloading 1380 of 1391 images...\n",
      "downloading 1381 of 1391 images...\n",
      "downloading 1382 of 1391 images...\n",
      "downloading 1383 of 1391 images...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading 1384 of 1391 images...\n",
      "downloading 1385 of 1391 images...\n",
      "downloading 1386 of 1391 images...\n",
      "downloading 1387 of 1391 images...\n",
      "downloading 1388 of 1391 images...\n",
      "downloading 1389 of 1391 images...\n",
      "downloading 1390 of 1391 images...\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "\n",
    "# Adding information about user agent\n",
    "opener=urllib.request.build_opener()\n",
    "opener.addheaders=[('User-Agent','Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1941.0 Safari/537.36')]\n",
    "urllib.request.install_opener(opener)\n",
    "\n",
    "for i in range(912,df.shape[0]):\n",
    "    dl_url = df.loc[i,\"url\"]\n",
    "    temp_label = df.loc[i,\"is_rep\"]\n",
    "    dl_name = dl_url.split(\"/\")[-1]\n",
    "\n",
    "    #retrieve image and save in images folder\n",
    "    print(f\"downloading {i} of {df.shape[0]} images...\")\n",
    "    urllib.request.urlretrieve(dl_url,f\"./datasets/images/{temp_label}/{dl_name}\")\n",
    "    \n",
    "    \n",
    "#credit: https://towardsdatascience.com/how-to-download-an-image-using-python-38a75cfa21c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#break to prevent run all\n",
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./datasets/url.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
