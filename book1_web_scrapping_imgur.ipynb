{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Project  - Book 1: Classifying real vs fake sneakers via images\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "The sneaker resale market is an estimated 2 billion USD in secondary market in 2019. Estimated to be USD6 billion by 2025 according to research firm, Cowen & Co. Due to the lucrative nature of these commodities, there is the inevitible rise of counterfeits. The counterfeit sneakers industry are a USD450 million market and we want to be able to differentiate real and fake sneakers. \n",
    "\n",
    "Our task is to build a classifier that is able to differentiate between real and fake sneakers. \n",
    "Our primary audience will be the sneaker brands. Some of the negative impacts of counterfeit sneakers includes undercutting sales of brands, damaging reputation and dealing with the lashback from consumers.\n",
    "\n",
    "To do so, we will first be scrapping data from reddit and other sneaker resources and using classification models such as CNN and xxxx to diffentiate between the authentic and the replicas. We will measure our success using several classification metrics including xxxx and yyyy. \n",
    "\n",
    "With this, we also hope to help buyers inform themselves and to stay away from counterfeits. Empowering the public with information, they will be able to make the right decision which could help to reduce the lucrative nature of fake sneakers. \n",
    "\n",
    "\n",
    "## Executive Summary\n",
    "\n",
    "As the data science team in Nutrino, we have been tasked to build a classifier to improve core product of the company, which is to provide nutrition related data services and analytics. We are also tasked to identify patterns on 2 currently trending diets, keto and vegan. \n",
    "\n",
    "Our classifier was successful in predicting at an above 90% accuracy score. We also identified patterns in the motivations and preferences of the 2 groups of subredditors, which will help determine the kind of customer engagement with teach group. \n",
    "\n",
    "\n",
    "## Notebooks:\n",
    "- [Data Scrapping and Cleaning](./book1_data_scrapping_cleaning.ipynb)\n",
    "- [EDA](./book2_eda.ipynb)\n",
    "- [Modeling and Recommendations](./book3_preprocesing_modeling_recommendations.ipynb)\n",
    "\n",
    "\n",
    "## Contents:\n",
    "- [Import Libraries](#Import-Libraries)\n",
    "- [Data Scrapping](#Data-Scrapping)\n",
    "- [Data Cleaning](#Data-Cleaning)\n",
    "- [Save Data to CSV](#Save-Data-to-CSV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Scrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get data from subreddits\n",
    "\n",
    "Lucky for us, imgur is able to display images grouped by subreddit. We will be using their json API to retrieve links for the images we need. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrieving url from page 1 of repsneakers...\n",
      "retrieving url from page 2 of repsneakers...\n",
      "retrieving url from page 3 of repsneakers...\n",
      "retrieving url from page 1 of sneakerreps...\n",
      "retrieving url from page 2 of sneakerreps...\n",
      "retrieving url from page 3 of sneakerreps...\n",
      "retrieving url from page 1 of wengkksneakers...\n",
      "retrieving url from page 2 of wengkksneakers...\n",
      "retrieving url from page 3 of wengkksneakers...\n",
      "retrieving url from page 1 of chanzhfsneakers...\n",
      "retrieving url from page 2 of chanzhfsneakers...\n",
      "retrieving url from page 3 of chanzhfsneakers...\n",
      "retrieving url from page 1 of michaelsneakers...\n",
      "retrieving url from page 2 of michaelsneakers...\n",
      "retrieving url from page 3 of michaelsneakers...\n",
      "retrieving url from page 1 of sneakermarket...\n",
      "retrieving url from page 2 of sneakermarket...\n",
      "retrieving url from page 3 of sneakermarket...\n",
      "retrieving url from page 1 of sneakers...\n",
      "retrieving url from page 2 of sneakers...\n",
      "retrieving url from page 3 of sneakers...\n",
      "retrieving url from page 1 of sneakerhead...\n",
      "retrieving url from page 2 of sneakerhead...\n",
      "retrieving url from page 3 of sneakerhead...\n",
      "retrieving url from page 1 of kicksmarket...\n",
      "retrieving url from page 2 of kicksmarket...\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'images'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-fee6a86f2515>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m                     \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malbum_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'hash'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                     \u001b[0mj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m                     \u001b[0;32mfor\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'images'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m#iterate through album\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ext'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'.gif'\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ext'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'.mp4'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                             \u001b[0;32mpass\u001b[0m                         \u001b[0;31m#if its a gif or video, pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'images'"
     ]
    }
   ],
   "source": [
    "#give list of sub reddits\n",
    "sub_reds = [\"repsneakers\",\"sneakerreps\", \"wengkksneakers\",\n",
    "            \"chanzhfsneakers\",\"michaelsneakers\",\"sneakermarket\",\n",
    "            \"sneakers\",\"sneakerhead\",\"sneakerscanada\"]\n",
    "\n",
    "#create lists to store scrapped data\n",
    "image_url = []\n",
    "rep_label = []\n",
    "\n",
    "#save links to variables\n",
    "imgur     = 'http://i.imgur.com/{}{}'\n",
    "page_api  = 'http://imgur.com/r/{}/new/page/{}/hit.json'\n",
    "album_api = 'http://imgur.com/ajaxalbums/getimages/{}/hit.json'\n",
    "\n",
    "\n",
    "for sub_red in sub_reds:                                 #iterate through the list of sub_reds\n",
    "    s = requests.session()                               #instantiate session\n",
    "    s.headers['user-agent'] = 'Mozilla/5.0'\n",
    "\n",
    "    for i in range(5):                                   #iterate through pages\n",
    "        url = page_api.format(sub_red,i)\n",
    "        print(f\"retrieving url from page {i+1} of {sub_red}...\")\n",
    "\n",
    "        j = s.get(url).json()\n",
    "        for entry in j['data']:                          #iterate through post in each page\n",
    "            if entry['ext'] == '.gif' or entry['ext'] == '.mp4':\n",
    "                pass                                     #if its a gif or video, pass\n",
    "            else:\n",
    "                if entry['is_album']:                    #check if its an album\n",
    "                    url = album_api.format(entry['hash'])\n",
    "                    j = s.get(url).json()\n",
    "                    for image in j['data']['images']:    #iterate through album\n",
    "                        if entry['ext'] == '.gif' or entry['ext'] == '.mp4':\n",
    "                            pass                         #if its a gif or video, pass\n",
    "                        else:\n",
    "                            url = imgur.format(image['hash'], image['ext'])\n",
    "                            image_url.append(url)        #if not append link to list\n",
    "                            rep_label.append(sub_red)    #add label\n",
    "                else:\n",
    "                    url = imgur.format(entry['hash'], entry['ext'])\n",
    "                    image_url.append(url)\n",
    "                    rep_label.append(sub_red)\n",
    "            \n",
    "\n",
    "#credit: https://kaijento.github.io/2017/05/08/web-scraping-imgur.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7860\n",
      "7860\n"
     ]
    }
   ],
   "source": [
    "#here we check the number of urls we have scrapped\n",
    "print(len(image_url))\n",
    "print(len(rep_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7860, 2)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(list(zip(image_url,rep_label)),columns=['url','label'])\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sneakerreps        2374\n",
       "repsneakers        1962\n",
       "michaelsneakers     916\n",
       "chanzhfsneakers     889\n",
       "kicksmarket         709\n",
       "sneakermarket       425\n",
       "sneakers            299\n",
       "wengkksneakers      195\n",
       "sneakerhead          91\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation 1 of the dataset\n",
    "We have a severely imbalanced class in repsneakers here. I will attempt to dig deeper into the sneakers and sneaker market subreddits for more images. \n",
    "\n",
    "This is likely because in r/repsneakers there are alot of people looking to \"quality control\" (QC) for the best replicas. Therefore, there are going to be more images and with better details. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "repsneakers        3583\n",
       "sneakerreps        3571\n",
       "michaelsneakers     916\n",
       "chanzhfsneakers     888\n",
       "kicksmarket         709\n",
       "sneakermarket       625\n",
       "sneakers            499\n",
       "wengkksneakers      195\n",
       "sneakerhead          91\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop_duplicates('url')['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation 2 of the dataset\n",
    "\n",
    "There seems to be alot of duplicates in the data. I am running the scrapper to see if my dataset increases or if there is a limit to the number of images availbale for scrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "# Adding information about user agent\n",
    "opener=urllib.request.build_opener()\n",
    "opener.addheaders=[('User-Agent','Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1941.0 Safari/537.36')]\n",
    "urllib.request.install_opener(opener)\n",
    "\n",
    "#change the image index where necessary\n",
    "for url in df['url']:\n",
    "    dl_url = url\n",
    "    dl_name = dl_url.split(\"/\")[-1]\n",
    "\n",
    "    #retrieve image and save in images folder\n",
    "    urllib.request.urlretrieve(dl_url,f'./datasets/images/{dl_name}')\n",
    "    \n",
    "    \n",
    "#credit: https://towardsdatascience.com/how-to-download-an-image-using-python-38a75cfa21c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./datasets/url.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
